%!TEX ROOT=geometria1.tex

\section{Spazio vettoriale}%
\label{sec:spazio_vettoriale}

\begin{Def}{Spazio vettoriale}
  Un insieme $V$ si definisce spazio vettoriale sul campo $\mathbb{K}$ se sono definite
  su $V$ due operazioni
  \begin{enumerate}
    \item \textbf{Somma} definita come
      \begin{align*}
        +:\,&V\times V\to V\\
            &(\vec{x},\vec{y})\mapsto\vec{x}+\vec{y}
      \end{align*}
      rispetto alla quale $(V,\,+)$ ha la struttura di gruppo commutativo. Ovvero
      \begin{enumerate}
        \item $\vec{x}+\vec{y} = \vec{y}+\vec{x}$
        \item $(\vec{x}+\vec{y})+\vec{z} = \vec{x} + (\vec{y}+\vec{z})$
        \item $\exists\vec{o}\in V\suchthat \vec{x}+\vec{o}=\vec{x}$ e si definisce
          $\vec{o}$ vettore nullo.
        \item $\forall\vec{x}\in V\,\exists\vec{x}\in
          V\suchthat\vec{x}+(-\vec{x})=\vec{o}$ e si definisce opposto.
      \end{enumerate}
    \item \textbf{Prodotto} definito per uno scalare
      \begin{align*}
        &\mathbb{K}\times V\to V\\
        &(\lambda,\vec{x})\mapsto \lambda\vec{x}
      \end{align*}
      e si ha che
      \begin{enumerate}
        \item $\lambda(\vec{x}+\vec{y}) = \lambda\vec{x}+\lambda\vec{y}$
        \item $(\lambda+\mu)\vec{x} = \lambda\vec{x}+\mu\vec{x}$
        \item $(\lambda\mu)\vec{x} = \lambda(\mu\vec{x})$
        \item $1\vec{x} = \vec{x}$
      \end{enumerate}
  \end{enumerate}
\end{Def}

\begin{Def}{Eleementi dello spazio}
  Gli elementi di $V$ sono detti vettori, quelli di $\mathbb{K}$ scalari.
\end{Def}

\begin{Def}{Campo}
  Un campo è un insieme i cui elementi sono detti numeri, che contiene $0$ e $1$ e ha
  due operazioni $+$ e $\cdot$ che verificano
  \begin{multicols}{2}
    \begin{enumerate}
      \item $\alpha+\beta = \beta+\alpha$
      \item $\alpha+(\beta+\gamma) = (\alpha+\beta)+\gamma$
      \item $\alpha+0 = \alpha$
      \item $\alpha+(-\alpha)=0$
      \columnbreak%
      \item $\alpha\beta = \beta\alpha$
      \item $(\alpha\beta)\gamma = \alpha(\beta\gamma)$
      \item $1\alpha = \alpha$
      \item $\alpha\alpha^{-1}=1$ se $\alpha\neq0$
      \item $(\alpha+\beta)\gamma = \alpha\gamma+\beta\gamma$
    \end{enumerate}
  \end{multicols}
\end{Def}

\subsection{Spazi particolari}%
\label{sub:spazi_particolari}

In generale $\mathbb{R}^n$ è uno spazio vettoriale, così come anche in generale
$\mathbb{K}^n$. Infatti si ha che
\begin{equation*}
  (x_1,\ldots,x_2)+(y_1,\ldots,y_n) = (x_1+y_1,\ldots,x_n+y_n)
\end{equation*}
e
\begin{equation*}
  \lambda(x_1,\ldots,x_n) = (\lambda x_1,\ldots,\lambda x_n)
\end{equation*}
In generale anche $\mathbb{K}^{m,n}$ è uno spazio vettoriale (e quindi anche
$\mathbb{R}^{m,n}$).\\
Il più piccolo spazio vettoriale è quello composto dal solo vettore nullo, ovvero
$\{\vec{o}\}$.\
Un caso particolare è lo spazio dei polinomi reali in $x$, denotato come $\mathbb{R}[x]$
che è
\begin{equation*}
  \mathbb{R}[x]\bydef \left\{ a_0+a_1x+\cdots+a_n x^n\Setsuchthat
  n\in\mathbb{N},\,a_i\in\mathbb{R},\,i=0,\ldots,n \right\}
\end{equation*}
È anche interessante il caso in cui si consideri l'insieme
\begin{equation*}
  \mathscr{F} = \left\{ f:\,\mathbb{R}\to\mathbb{R}\qq{funzione} \right\}
\end{equation*}
in quanto anche questo è uno spazio vettoriale infatti
\begin{equation*}
  (f+g)(x)\bydef f(x)+g(x)
\end{equation*}
e
\begin{equation*}
  (\lambda f)(x)\bydef \lambda f(x)
\end{equation*}

\subsection{Proprietà formali}%
\label{sub:proprieta_formali}

In un campo vettoriale su $\mathbb{K}$ valgono le seguenti proprietà
\begin{enumerate}
  \item Vettore nullo unico
    \begin{proof}
      Supponiamo per assurdo che esistano $\vec{o}$ e $\vec{o'}$ nulli in modo che
      $\vec{o}\neq\vec{o'}$. Allora si ha $\vec{o}=\vec{o}+\vec{o'}$ sfruttando il fatto
      che $\vec{o'}$ è un vettore nullo. Analogamente si ha che
      $\vec{o'}=\vec{o'}+\vec{o}$. Da queste due relazioni si deduce che
      $\vec{o'}=\vec{o}$ che va contro l'ipotesi iniziale.
    \end{proof}
  \item Opposto unico
    \begin{proof}
      Supponiamo per assurdo che esistano $\vec{x_1}\neq\vec{x_2}$ oposti di $\vec{x}$.
      Allora possiamo scrivere $(\vec{x}+\vec{x_1})+\vec{x_2} =
      \vec{o}+\vec{x_2}=\vec{x_2}$. Analogamente si ha che
      $(\vec{x}+\vec{x_1})+\vec{x_2}=\vec{x}+(\vec{x_2}+\vec{x_1})=(\vec{x}+\vec{x_2})
      +\vec{x_1}=\vec{o}+\vec{x_1}=\vec{x_1}$. Si deduce quindi che
      $\vec{x_1}=\vec{x_2}$ ma per ipotesi questo non può essere.
    \end{proof}
  \item Se per $\vec{x}$, $\vec{y}$, $\vec{z}$ si ha $\vec{x}+\vec{y}=\vec{x}+\vec{z}$
    allora $\vec{y}=\vec{z}$
    \begin{proof}
      La dimostrazione segue direttamente dalla seconda proprietà, infatti si può
      aggiungere $-\vec{x}$ ad entrambi i membri e ottenere
      $\vec{x}+\vec{y}-\vec{x}=\vec{x}+\vec{z}-\vec{x}$. Si ottiene
      $\vec{o}+\vec{y}=\vec{z}+\vec{o}$ e infine $\vec{y}=\vec{z}$.
    \end{proof}
  \item Solo su $\mathbb{R}$ vale che $\lambda\vec{x}=\vec{o}$ con
    $\lambda\in\mathbb{R}$ allora $\lambda=0\lor\vec{x}=\vec{o}$
    \begin{proof}
      Essendo una biimplicazione, bisogna dimostrare entrambi i versi. Dimostriamo
      $\Leftarrow$. Possiamo provare che $0\vec{x}\vec{o}$ e $\lambda\vec{o}=\vec{o}$.
      Per il primo caso si può dire che $0\vec{x}=(0+0)\vec{x}=0\vec{x}+0\vec{x}$. Per
      il punto precedente, abbiamo che $o\vec{x}=o\vec{x}+o\vec{x}$ e semplificando si
      ottiene $\vec{o}=o\vec{x}$. Il secondo caso si dimostra analogamente
      $\lambda\vec{o}=\lambda(\vec{o}+\vec{o})=\lambda\vec{o}+\lambda\vec{o}$. Per il
      punto precedente $\lambda\vec{o}=\vec{o}\lambda+\lambda\vec{o}$, semplificando
      $\vec{0}=\lambda\vec{o}$.\\
      L'altro vers ($\Rightarrow$) dice che $\lambda\vec{x}=\vec{o}$. Se $\lambda=0$ è
      immediato. Se $\lambda\neq0$, sicuramente $\exists\lambda^{-1}$. Possiamo allora
      scrivere $\vec{o}=\lambda^{-1}\vec{o}=\lambda^{-1}(\lambda\vec{o}\vec{x})=
      (\lambda^{-1}\lambda)\vec{x}=\vec{x}$.
    \end{proof}
  \item $(-1)\vec{x} = -\vec{x}$
    \begin{proof}
      Si ha che $\vec{x}+(-1)\vec{x}=1\vec{x}+(-1)\vec{x}=(1-1)\vec{x}=\vec{o}$.
    \end{proof}
\end{enumerate}

\subsection{Sottoinsiemi di spazi vettoriali}%
\label{sub:sottoinsiemi_di_spazi_vettoriali}

\begin{Def}{Sottospazio vettoriale}
  Sia $V$ uno spazio vettoriale su $\mathbb{K}$. Un sottoinsieme $W$ di $V$ è un
  sottospazio vettoriale di $V$ se $W$ è uno spazio vettoriale rispetto alle stesse
  operazioni di $V$, ovvero rispetto alla somma e al prodotto per scalari. Formalmente
  se vale
  \begin{equation*}
    \forall\lambda,\mu\in\mathbb{K}\;\forall\vec{x},\vec{y}\in W\quad
    \lambda\vec{x}+\mu\vec{y}\in W
  \end{equation*}
\end{Def}
Si noti che $(W,+)$ è un sottgruppo di $V$ rispetto alla somma. Si noti anche che il
vettore nullo di $V$ appartiene ad ogni sottospazio vettoriale $W$ di $V$, infatti
$\lambda\vec{x}\in W\quad\lambda=0\implies\lambda\vec{x}=\vec{o}\in W$.

\begin{SubDef}{Sottospazi impropri}
  Ogni spazio vettoriale ha almeno due sottospazi vettoriali: se stesso e $\{\vec{o}\}$.
\end{SubDef}

Si noti anche che se $W$ è un sottospazio vettoriale, $\vec{x}\in W\implies-\vec{x}\in
W$.

\subsubsection{Esempio fondamentale di sottospazio vettoriale}%
\label{ssub:esempio_fondamentale_di_sottospazio_vettoriale}

Si prenda l'insieme delle soluzioni di un sistema lineare omogeneo di $m$ equazioni in
$n$ incognite. L'insieme è un sottospazio vettoriale di $\mathbb{R}^n$. In generale
l'insieme di soluzioni di $AX=B$ è un sottospazio vettoriale di $\mathbb{R}^n$ se e solo
se il sistema è omogeneo.

\begin{Def}{Nullspace}
  Sia $AX=O$ un sistema lineare omogeneo con $A\in\mathbb{R}^{m,n}$ e
  $X=
  \begin{pmatrix}
    X_1\\\vdots\\X_n
  \end{pmatrix}\in\mathbb{R}^{n,1}$. Allora
  \begin{equation*}
    N(A)\bydef \left\{ X\in\mathbb{R}^{n,1}\Setsuchthat AX = O
    \right\}\subseteq\mathbb{R}^n
  \end{equation*}
  si definisce \textbf{nullspace} di $A$ che contiene l'insieme delle soluzioni.
\end{Def}

Il nullspace è uno sottospazio vettoriale in quanto
$\forall\lambda,\mu\in\mathbb{R}\;\forall X,Y\in N(A)\quad \lambda X+\mu Y\in N(A)$.
Infatti si ha che $A(\lambda X+\mu Y) = O = \lambda AX+\mu (AY)$ in quanto sia $X$ che
$Y$ sono soluzioni.

\subsubsection{Esempi di sottospazi vettoriali nello spazio delle matrici}%
\label{sub:esempi_di_sottospazi_vettoriali_nello_spazio_dell_matrici}

\begin{Def}{Insieme delle matrici diagonali}
  \begin{equation*}
    \mathscr{D}\left( \mathbb{R}^{n,n} \right) \bydef \left\{ D =
      \begin{pmatrix}
        d_1 & \cdots & 0\\
        0 & \ddots & \vdots\\
        0 & 0 & d_n
      \end{pmatrix}\in\mathbb{R}^{n,n}
      \Setsuchthat d_i\in\mathbb{R}^{n,n}
    \right\}
  \end{equation*}
  È uno sottospazio vettoriale in quanto combinazioni lineari di matrici diagonali, sono
  ancora matrici diagonali.
\end{Def}

\begin{Def}{Insieme delle matrici triangolari superiori e inferiori}
    \begin{equation*}
      \tau \left( \mathbb{R}^{n,n} \right)\bydef \left\{
        \begin{pmatrix}
          a_{11} & \cdots & \cdots & a_{1n}\\
          0 & a_{22} & \cdots & a_{2n}\\
          \vdots & \cdots & \ddots & \vdots\\
          0 & \cdots & 0 & a_{nn}
        \end{pmatrix}\in\mathbb{R}^{n,n}
        \Setsuchthat a_{ij}\in\mathbb{R}
      \right\}
    \end{equation*}
    e
    \begin{equation*}
      \tau \left( \mathbb{R}^{n,n} \right)\bydef \left\{
        \begin{pmatrix}
          a_{11} & 0 & \cdots & 0\\
          \vdots & a_{22} & 0 & \vdots\\
          \vdots & \cdots & \ddots & \vdots\\
          a_{n1} & \cdots & \cdots & a_{nn}
        \end{pmatrix}\in\mathbb{R}^{n,n}
        \Setsuchthat a_{ij}\in\mathbb{R}
      \right\}
    \end{equation*}
    sono sottospazi vettoriali di $\mathbb{R}^{n,n}$.
\end{Def}

\begin{Def}{Insieme delle matrici simmetriche}
  \begin{equation*}
    \mathscr{S}\left( \mathbb{R}^{n,n} \right)\bydef \left\{A\in\mathbb{R}^{n,n}\Setsuchthat
    \transp A = A\right\}
  \end{equation*}
  è uno sottospazio vettoriale di $\mathbb{R}^{n,n}$.
\end{Def}

\begin{Def}{Insieme delle matrici antisimmetriche}
  \begin{equation*}
    \mathscr{S}\left( \mathbb{R}^{n,n} \right)\bydef \left\{A\in\mathbb{R}^{n,n}\Setsuchthat
    \transp A = -A\right\}
  \end{equation*}
  è un sottospazio vettoriale di $\mathbb{R}^{n,n}$.
\end{Def}

\begin{Def}{Insieme delle matrici ortogonali reali}
  \begin{equation*}
    \mathscr{O}(n,\mathbb{R})\bydef \left\{ A\in\mathbb{R}^{n,n}\Setsuchthat A\transp A
    = I = \transp A A\right\}
  \end{equation*}
  \textbf{non} è uno sottospazio vettoriale di $\mathbb{R}^{n,n}$ in quanto $O\notin
  O(n,\mathbb{R})$.
\end{Def}

\subsection{Combinazioni lineari}%
\label{sub:combinazioni_lineari}

\begin{Def}{Combinazione lineare}
  Dati $l$ vettori $\vec{v_1},\ldots,\vec{v_l}$ di uno spazio vettoriale $V$ su
  $\mathbb{K}$, si dice che un vettore $\vec{x}$ è una combinazione lineare dei vettori
  $\vec{v_1},\ldots,\vec{v_l}$ se esistono $x_1,\ldots,x_l\in\mathbb{K}$ tali che
  $\vec{x}=x_1\vec{v_1}+\cdots+x_l\vec{v_l}$. $x_i$ si dice coefficiente.
\end{Def}

\begin{Def}{Insieme delle combinazioni lineari}
  Fissando i vettori $\vec{v_1},\ldots,\vec{v_2}$, si definisce
  \begin{equation*}
    \mathscr{L}(\vec{v_1},\ldots,\vec{v_l})\bydef \left\{
    x_1\vec{v_1}+\cdots+x_l\vec{v_l} \Setsuchthat x_i\in\mathbb{K},\;i=1,\ldots,l \right\}
  \end{equation*}
\end{Def}

\begin{Def}{Sistema di generatori di $\mathscr{L}(\vec{v_1},\ldots,\vec{v_l})$}
  Il sistema di generatori è l'insieme $\{x_1\vec{v_1},\ldots,x_l\vec{v_l}\}$.
\end{Def}

\begin{Thm}{Sottospazio delle combinazioni lineari}
  $\mathscr{L}(\vec{v_1},\ldots,\vec{v_l})$ è un sottospazio vettoriale di $V$ ed è il
  più piccolo sottospazio vettoriale di $V$ a contenere i vettori
  $\vec{v_1},\ldots,\vec{v_l}$.
\end{Thm}

\begin{Def}{Sistema di generatori di un sottospazio}
  Siano $\vec{v_1},\ldots,\vec{v_l}$ vettori di $V$. Si dice che un sottospazio
  vettoriale $W$ di $V$ ha come sistema di generatori $\{\vec{v_1},\ldots,\vec{v_l}\}$
  se $W=\mathscr{L}(\vec{v_1},\ldots,\vec{v_l})$.
\end{Def}

\begin{Thm}{Modifiche ai generatori}
  Detto $W=\mathscr{L}(\vec{v_1},\ldots,\vec{v_l})$, si possono aggiungere o sostituire
  più generatori di $W$ con loro combinazioni lineari.
\end{Thm}

Come conseguenza di questo teorema si ha che $W=\mathscr{L}(\vec{v_1},\ldots,\vec{v_l})$
ha infiniti sistemi generatori.

\begin{Def}{Spazi finitiamente generati}
  Uno spazio vettoriale $V$ si dice finitamente generato se esistono $l$ vettori
  $\vec{v_1},\ldots,\vec{v_l}$ di $V$ tali che $V=\mathscr{L}(\vec{v_1},\ldots,\vec{v_l}
  )$.
\end{Def}

\begin{Def}{Sottospazi finitiamente generati}
  Un sottospazio vettoriale $W$ si dice finitamente generato se esistono $l$ vettori
  $\vec{v_1},\ldots,\vec{v_l}$ di $W$ tali che $W=\mathscr{L}(\vec{v_1},\ldots,\vec{v_l}
  )$.
\end{Def}

\subsubsection{Esempi di spazi finitamente generati}%
\label{ssub:esempi_di_spazi_finitamente_generati}

$\mathbb{R}^n$ è finitamente generato, in quanto possiamo definire
$\vec{e_1}=(1,0,\ldots,0)$, $\vec{e_i}=(0,\ldots,1,\ldots,0)$ dove l'$1$ è all'$i$-esimo
posto e $\vec{e_n}=(0,\ldots,1)$. In questo modo una qualsiasi $n$-upla la si può
scrivere come $(x_1,\ldots,x_n)=x_1\vec{e_1}+\cdots+x_n\vec{e_n}$.\\
Analogamente anche $\mathbb{R}^{m,n}$ è finitamente generato, creando delle matrici
nello stesso modo.

\subsubsection{Dipendenza lineare}%
\label{sub:dipendenza_lineare}

\begin{Def}{Vettori linearmente indipendenti}
  Dati $l$ vettori $\vec{v_1},\ldots,\vec{v_l}$ di $V$ su $\mathbb{K}$, si dicono
  linearmente indipendenti se l'unica loro combinazioni lineare uguale a $\vec{o}$ è
  quella che ha coefficienti tutti nulli.
\end{Def}

\begin{SubDef}{Insieme libero}
  L'insieme di vettori linearmente indipendenti è un insieme libero.
\end{SubDef}

\begin{Def}{Vettori linearmente dipendenti}
  Dati $l$ vettori $\vec{v_1},\ldots,\vec{v_l}$ di $V$ su $\mathbb{K}$, si dicono
  linearmente dipendenti se esiste almeno una combinazione lineare uguale a $\vec{o}$ a
  coefficienti non tutti nulli.
\end{Def}

\begin{Thm}{Dipendenza lineare e combinazioni lineari}
  Dati $l$ vettori $\vec{v_1},\ldots,\vec{v_l}$ di $V$ su $\mathbb{K}$ essi sono
  linearmente dipendenti se e solo se uno è combinazione lineare degli altri.
\end{Thm}

\begin{proof}
  Essendo un se e solo se, si devono dimostrare entrambe le implicazioni. Dimostrando
  $\Rightarrow$, si può dire per ipotesi che i vettori sono linearmente indipendenti, e
  quindi
  \begin{equation*}
    \exists x_1\vec{v_1}+\cdots+x_l\vec{v_l}=\vec{o}\qq{con} x_1\neq0
  \end{equation*}
  Isolando $\vec{v_1}$ si dimostra
  \begin{equation*}
    \vec{v_1}= - \frac{x_2}{x_1}\vec{v_1}-\cdots-\frac{x_l}{x_1}\vec{v_l}
  \end{equation*}
  L'altra implicazione ($\Leftarrow$) si dimostra anlogamente. Per ipotesi se
  \begin{equation*}
    \vec{v_i}=\lambda_1\vec{v_1}+\cdots+\lambda_{i-1}\vec{v_{i-1}}+
    \lambda_{i+1}\vec{v_{i+1}}+\cdots+\lambda_l\vec{v_l}
  \end{equation*}
  allora
  \begin{equation*}
    \vec{o} =
    \lambda_1\vec{v_1}+\cdots\lambda_{i-1}\vec{v_i-1}-\vec{v_i}+\lambda_{i+1}\vec{v_i+1}
    +\cdots+\lambda_l\vec{v_l}
  \end{equation*}
\end{proof}

\subsection{Base di uno spazio vettoriale}%
\label{sub:base_di_uno_spazio_vettoriale}

\begin{Def}{Base}
  Un insieme finito e ordinato di $V$ denotato con
  $\mathscr{B}(\vec{v_1},\ldots,\vec{v_n})$ è detto base di $V$ se è insieme libero e un
  sistema di generatori.
\end{Def}

\begin{SubDef}{Basi canoniche o standard}
  In $\mathbb{R}^n$,
  \begin{equation*}
    \mathscr{B}\bigl((1,0,\ldots,0),(0,1,0,\ldots,0),\ldots,(0,\ldots,0,1)\bigr)
  \end{equation*}
  è detto base canonica o standard.\\[\baselineskip]
  In $\mathbb{R}^{m,n}$ la base canonica o standard è
  \begin{equation*}
    \mathscr{B} \left(
      \begin{pmatrix}
        1 & 0 &\cdots\\
        0 & 0 & \cdots\\
        \vdots & \vdots & \cdots
      \end{pmatrix},\ldots,
      \begin{pmatrix}
        0 & \cdots & 0\\
        \vdots & 1 & \vdots\\
        0 & \cdots & 0
      \end{pmatrix}
    \right)
  \end{equation*}
  Ovvero sono le matrici che al posto di indici $_{ij}$ è $1$, ovunque è
  $0$.\\[\baselineskip]
  Su $\mathbb{R}_n[x]$ (ovvero l'insieem dei polinomi reali in $x$ con grado minore o
  uguale a $n$) una base canonica o standard è $(1,x,x^2,\ldots,x^n)$.
\end{SubDef}
